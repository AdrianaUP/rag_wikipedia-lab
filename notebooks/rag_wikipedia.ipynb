{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f055f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pipeline(\"text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "docs = retriever.get_relevant_documents(\n",
    "    \"Summarize federated learning healthcare challenges and opportunities.\"\n",
    ")\n",
    "context = \"\\n\\n\".join([d.page_content[:1200] for d in docs])\n",
    "\n",
    "prompt = (\n",
    "    \"Use only the provided context from Wikipedia to write a factual Markdown summary (400–500 words) \"\n",
    "    \"with the exact headers: Introduction, Key Findings, Ethical & Technical Challenges, Conclusion.\\n\\n\"\n",
    "    f\"Context:\\n{context}\\n\\nWrite the summary now:\"\n",
    ")\n",
    "\n",
    "out = gen(prompt, max_length=950, do_sample=False)\n",
    "summary_md = out[0][\"generated_text\"]\n",
    "\n",
    "with open(OUTPUT_DIR / \"rag_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(\"Resumen guardado → outputs/rag_summary.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d896b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"wiki_ai\",\n",
    "    embedding_function=hf_embeddings,\n",
    "    persist_directory=str(DATA_DIR / \"chroma_db\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "query_examples = [\n",
    "    \"Explain federated learning challenges in healthcare.\",\n",
    "    \"List key privacy risks in federated learning.\",\n",
    "    \"How does federated learning differ from centralized training?\"\n",
    "]\n",
    "\n",
    "retrieval_results = {}\n",
    "for q in query_examples:\n",
    "    docs = retriever.get_relevant_documents(q)\n",
    "    retrieval_results[q] = [\n",
    "        {\"source\": d.metadata, \"snippet\": d.page_content[:500]}\n",
    "        for d in docs\n",
    "    ]\n",
    "\n",
    "with open(OUTPUT_DIR / \"retrieval_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(retrieval_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Ejemplos de recuperación guardados → outputs/retrieval_examples.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "client = chromadb.Client(Settings(\n",
    "    persist_directory=str(DATA_DIR / \"chroma_db\"),\n",
    "    chroma_db_impl=\"duckdb+parquet\"\n",
    "))\n",
    "collection = client.create_collection(\"wiki_ai\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "ids = df[\"id\"].tolist()\n",
    "metadatas = [{\"title\": t} for t in df[\"title\"].tolist()]\n",
    "documents = df[\"text\"].tolist()\n",
    "\n",
    "collection.upsert(documents=documents, metadatas=metadatas, ids=ids)\n",
    "client.persist()\n",
    "print(f\"{len(documents)} documentos insertados en ChromaDB → data/chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia('en')\n",
    "page = wiki.page(PAGE_TITLE)\n",
    "if not page.exists():\n",
    "    raise ValueError(f\"Wikipedia page '{PAGE_TITLE}' not found.\")\n",
    "\n",
    "text = page.text.strip()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=150, separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "records = [{\"id\": f\"{PAGE_TITLE}_{i+1}\", \"title\": PAGE_TITLE, \"text\": chunk}\n",
    "           for i, chunk in enumerate(chunks)]\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df.to_csv(DATA_DIR / \"wiki_corpus.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Corpus guardado con {len(df)} chunks → data/wiki_corpus.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import wikipediaapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import pipeline\n",
    "\n",
    "# Forzar uso de CPU (evita errores de DLL)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Rutas\n",
    "DATA_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOPIC = \"Federated learning\"\n",
    "PAGE_TITLE = \"Federated_learning\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
